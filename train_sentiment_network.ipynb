{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_review_fp = open(\"positive_reviews.txt\", 'r')\n",
    "negative_review_fp = open(\"negative_reviews.txt\", 'r')\n",
    "\n",
    "#Load Positive Words\n",
    "positive_words_fp = open(\"positive-words.txt\", 'r')\n",
    "words = positive_words_fp.readlines()\n",
    "positive_word_list = []\n",
    "for word in words:\n",
    "    word = word.replace('\\n','')\n",
    "    positive_word_list.append(word)\n",
    "    \n",
    "\n",
    "#Load Negative Words\n",
    "negative_words_fp = open(\"negative-words.txt\", 'r')\n",
    "words = negative_words_fp.readlines()\n",
    "negative_word_list = []\n",
    "for word in words:\n",
    "    word = word.replace('\\n','')\n",
    "    negative_word_list.append(word)\n",
    "\n",
    "#print(positive_word_list)\n",
    "#print(negative_word_list)\n",
    "\n",
    "#Create Review Feature Dataset\n",
    "#----------------------------------\n",
    "#Read Positive Reviews\n",
    "train_data = []\n",
    "train_labels = []\n",
    "lines = positive_review_fp.readlines()\n",
    "for line in lines:\n",
    "    datarow = []\n",
    "    if line.strip():\n",
    "        tokens = line.strip('\\n').split(' ')\n",
    "        #print(tokens)\n",
    "        for token in tokens:\n",
    "            if token.lower() in positive_word_list:\n",
    "                datarow.append(1)\n",
    "            elif token.lower() in negative_word_list:\n",
    "                datarow.append(-1)\n",
    "            elif len(token) >= 4 and any(token.lower() in x for x in positive_word_list):\n",
    "                datarow.append(1)\n",
    "            elif len(token) >= 4 and any(token.lower() in x for x in negative_word_list):\n",
    "                datarow.append(-1)\n",
    "            else:\n",
    "                datarow.append(0)\n",
    "        #print(sum(datarow))\n",
    "        datarow.append(sum(datarow))\n",
    "        train_data.append(datarow)\n",
    "        train_labels.append(1)\n",
    "        \n",
    "#Read Negative Reviews\n",
    "lines = negative_review_fp.readlines()\n",
    "for line in lines:\n",
    "    datarow = []\n",
    "    if line.strip():\n",
    "        tokens = line.strip('\\n').split(' ')\n",
    "        #print(tokens)\n",
    "        for token in tokens:\n",
    "            if token.lower() in positive_word_list:\n",
    "                datarow.append(1)\n",
    "            elif token.lower() in negative_word_list:\n",
    "                datarow.append(-1)\n",
    "            elif len(token) >= 4 and any(token.lower() in x for x in positive_word_list):\n",
    "                datarow.append(1)\n",
    "            elif len(token) >= 4 and any(token.lower() in x for x in negative_word_list):\n",
    "                datarow.append(-1)\n",
    "            else:\n",
    "                datarow.append(0)\n",
    "        #print(sum(datarow))\n",
    "        datarow.append(sum(datarow))\n",
    "        train_data.append(datarow)\n",
    "        train_labels.append(-1)\n",
    "#print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create Test Feature Dataset \n",
    "#which contains tweets having \n",
    "#words occurring in suicidal tweets\n",
    "tweet_data = []\n",
    "tweet_labels = []\n",
    "tweets_fp = open(\"suicide_tweets_csv.csv\", 'r')\n",
    "lines = tweets_fp.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    datarow = []\n",
    "    if line.strip():\n",
    "        tokens = line.strip('\\n').split(',')\n",
    "        ##print(tokens)\n",
    "        for token in tokens:\n",
    "            if token.lower() in positive_word_list:\n",
    "                datarow.append(1)\n",
    "            elif token.lower() in negative_word_list:\n",
    "                datarow.append(-1)\n",
    "            elif len(token) >= 4 and any(token.lower() in x for x in positive_word_list):\n",
    "                datarow.append(1)\n",
    "            elif len(token) >= 4 and any(token.lower() in x for x in negative_word_list):\n",
    "                datarow.append(-1)\n",
    "            else:\n",
    "                datarow.append(0)\n",
    "        #print(sum(datarow))\n",
    "        datarow.append(sum(datarow))\n",
    "        tweet_data.append(datarow)\n",
    "        tweet_labels.append(1)\n",
    "        #print (datarow)\n",
    "        #print (tokens)\n",
    "#print(tweet_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_review_fp.close()\n",
    "negative_review_fp.close()\n",
    "positive_words_fp.close()\n",
    "negative_words_fp.close()\n",
    "tweets_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = max(len(l) for l in train_data)\n",
    "n_samples = len(train_data)\n",
    "#print(n_samples)\n",
    "\n",
    "X = [[0 for x in range(n_features)] for y in range(n_samples)]\n",
    "\n",
    "for listindex, listitem in enumerate(train_data):\n",
    "    for sublistindex, sublistitem in enumerate(listitem):\n",
    "        #print(listindex)\n",
    "        #print(sublistindex)\n",
    "        X[listindex][sublistindex] = sublistitem\n",
    "\n",
    "y = train_labels\n",
    "clf = MLPClassifier(solver='lbgfs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_features = max(len(l) for l in tweet_data)\n",
    "tweet_samples = len(tweet_data)\n",
    "#print(tweet_features)\n",
    "#print(n_features)\n",
    "\n",
    "test_X = [[0 for x in range(n_features)] for y in range(tweet_samples)]\n",
    "\n",
    "for listindex, listitem in enumerate(tweet_data):\n",
    "    for sublistindex, sublistitem in enumerate(listitem):\n",
    "        #print(listindex)\n",
    "        #print(sublistindex)\n",
    "        test_X[listindex][sublistindex] = sublistitem\n",
    "\n",
    "predicted_values = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, value in enumerate(predicted_values):\n",
    "    if index < 100:\n",
    "        pass\n",
    "        #print(lines[index] + \" : \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#print (negative_tweets,)\\n#print(negative_tweets_feature)\\ntweets_feature_km = []\\ntweets_feature_km_2d = []\\nfor item in negative_tweets_feature:\\n    tweets_feature_km.append(sum(item))\\n    if sum(item) < -10\\n    print (tweets_feature_km)\\n    \\n    #print( sum(item))\\n#print (tweets_feature_km)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets_feature = []\n",
    "negative_tweets = []\n",
    "negative_tweet = open('negative_tweets.txt','a',encoding='utf-8')\n",
    "for index, tweet in enumerate(test_X):\n",
    "    if predicted_values[index] < 0:\n",
    "        if sum(tweet) < -5:\n",
    "            #print (lines[index])\n",
    "            negative_tweet.write(lines[index]+'\\n')\n",
    "            \n",
    "            \n",
    "negative_tweet.close()\n",
    "\"\"\"\n",
    "        negative_tweets_feature.append(tweet)\n",
    "        negative_tweets.append(lines[index])\n",
    "        \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#print (negative_tweets,)\n",
    "#print(negative_tweets_feature)\n",
    "tweets_feature_km = []\n",
    "tweets_feature_km_2d = []\n",
    "for item in negative_tweets_feature:\n",
    "    tweets_feature_km.append(sum(item))\n",
    "    if sum(item) < -10\n",
    "    print (tweets_feature_km)\n",
    "    \n",
    "    #print( sum(item))\n",
    "#print (tweets_feature_km)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_weight_fp = open('tweet_weight.txt','a')\n",
    "suicide_terms_fp = open(\"suicide_term_and_weights.txt\", 'r')\n",
    "suicide_dict = {}\n",
    "stlines = suicide_terms_fp.readlines()\n",
    "\n",
    "for line in stlines:\n",
    "    tokens = line.strip('\\n').split(',')\n",
    "    suicide_dict[tokens[0]] = tokens[1]\n",
    "\n",
    "#print (suicide_dict)\n",
    "\n",
    "suicide_tweets_features = []\n",
    "for line in lines:\n",
    "    datarow = []\n",
    "    #if line.strip():\n",
    "    tokens = line.strip('\\n').split(',')\n",
    "    ##print(tokens)\n",
    "    for token in tokens:\n",
    "        if token.lower() in suicide_dict:\n",
    "            datarow.append(int(suicide_dict[token.lower()]))\n",
    "        else:\n",
    "            datarow.append(0)\n",
    "    #print(sum(datarow))\n",
    "    datarow.append(sum(datarow))\n",
    "    suicide_tweets_features.append(datarow)\n",
    "    #print (datarow)\n",
    "    #print (tokens)\n",
    "    tweet_weight_fp.write(str(datarow )+'\\n')\n",
    "    tweet_weight_fp.write(str(tokens)+'\\n' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_weight_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suicide_terms_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_weight_fp1 = open('tweet_weight_16.txt','w')\n",
    "for index, tweet in enumerate(suicide_tweets_features):\n",
    "    if sum(tweet) > 16:\n",
    "        #print (lines[index])\n",
    "        #print (suicide_tweets_features[index])\n",
    "        tweet_weight_fp1.write(lines[index] + '\\n')\n",
    "        tweet_weight_fp1.write(str(suicide_tweets_features[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_weight_fp1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for index, tweet in enumerate(suicide_tweets_features):\n",
    " #   if sum(tweet) > 16:\n",
    "       # print (lines[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
